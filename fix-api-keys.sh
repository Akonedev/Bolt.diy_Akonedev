#!/bin/bash

echo "â˜…â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â˜…"
echo "    Configuration des ClÃ©s API - Bolt.DIY"
echo "â˜…â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â˜…"
echo ""

echo "ðŸ”§ Le problÃ¨me : la clÃ© OpenRouter semble expirÃ©e ou invalide"
echo ""
echo "ðŸ“‹ Solutions disponibles :"
echo "   1. Utiliser Ollama en local (gratuit)"
echo "   2. Obtenir une nouvelle clÃ© OpenRouter"
echo "   3. Configurer d'autres providers gratuits"
echo ""

# Option 1: Configuration Ollama
echo "ðŸ”„ Configuration pour utiliser Ollama (local, gratuit)..."

# Mettre Ã  jour le .env.local avec Ollama comme provider principal
cat > .env.local << 'EOF'
# Configuration Docker pour Bolt.DIY - Ollama Local
# Ports personnalisÃ©s
PORT=44100
VITE_PORT=44100
PREVIEW_PORT=44200

# Configuration de base
VITE_LOG_LEVEL=debug
DEFAULT_NUM_CTX=32768
RUNNING_IN_DOCKER=true

# Provider principal : Ollama (local)
OLLAMA_API_BASE_URL=http://host.docker.internal:11434

# ClÃ©s API optionnelles (laissez vide si non utilisÃ©es)
GROQ_API_KEY=
HuggingFace_API_KEY=
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
OPEN_ROUTER_API_KEY=
GOOGLE_GENERATIVE_AI_API_KEY=
XAI_API_KEY=
TOGETHER_API_KEY=
TOGETHER_API_BASE_URL=
DEEPSEEK_API_KEY=
MISTRAL_API_KEY=
COHERE_API_KEY=
LMSTUDIO_API_BASE_URL=
PERPLEXITY_API_KEY=
HYPERBOLIC_API_KEY=
HYPERBOLIC_API_BASE_URL=
OPENAI_LIKE_API_BASE_URL=
OPENAI_LIKE_API_KEY=
AWS_BEDROCK_CONFIG=

# Configuration Docker spÃ©cifique
WRANGLER_SEND_METRICS=false
EOF

echo "âœ… Configuration mise Ã  jour pour utiliser Ollama"
echo ""
echo "ðŸ“¦ Pour installer Ollama si pas encore fait :"
echo "   curl -fsSL https://ollama.ai/install.sh | sh"
echo "   ollama pull llama2"  # ou un autre modÃ¨le
echo ""
echo "ðŸš€ Pour dÃ©marrer avec Docker maintenant :"
echo "   docker-compose up app-dev"
echo ""
echo "â„¹ï¸  L'application se connectera Ã  Ollama sur votre machine hÃ´te"
echo "   Assurez-vous qu'Ollama est dÃ©marrÃ© avec : ollama serve"